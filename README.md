

## **Welcome to my two-part series on setting up your computer to run Large Language Models (LLMs) locally!**

I've decided to split this guide into two parts because, let's face it, the NVIDIA drivers can be tricky to work with. In Part 1, we'll focus specifically on getting those drivers working correctly with Linux - an area where there's often conflicting information and trial-and-error is the norm. But don't worry, after some careful experimentation (and even a few close calls with my main Linux computer), I've managed to get it working consistently. And now, I'm excited to share those findings with you.

In Part 2, we'll take things up a gear by installing three popular AI frameworks: Stable-diffusion, Ollama, and Open WebUI. These frameworks are great examples of the diversity in AI tooling, and each has its own unique installation process. In addition to installing these frameworks, we'll also cover the roles that Docker, Cuda, and container toolkits play in helping you bring your AI projects to life. I've had a blast setting up my lab with these tools, and I hope this guide will help you get started on your AI journey too!